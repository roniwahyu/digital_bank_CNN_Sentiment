{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'webdriver_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Service\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwebdriver_manager\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChromeDriverManager\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'webdriver_manager'"
     ]
    }
   ],
   "source": [
    "# First cell: Install required libraries\n",
    "# !pip install requests beautifulsoup4 pandas selenium\n",
    "\n",
    "# Second cell: Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Third cell: Function to construct the Tokopedia search URL based on the query\n",
    "def construct_search_url(query):\n",
    "    base_url = \"https://www.tokopedia.com/search?st=product&q=\"\n",
    "    query = query.replace(' ', '+')  # Replace spaces with '+' for URL formatting\n",
    "    return base_url + query\n",
    "\n",
    "# Fourth cell: Scraping function using requests and BeautifulSoup\n",
    "def scrape_tokopedia(query):\n",
    "    url = construct_search_url(query)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the response is successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find product containers (You will need to inspect Tokopedia's page to get the right selectors)\n",
    "    products = soup.find_all('div', class_='css-1sn1xa2')  # Example class, inspect the real class name\n",
    "    \n",
    "    product_list = []\n",
    "    \n",
    "    # Loop through products and extract details\n",
    "    for product in products:\n",
    "        try:\n",
    "            name = product.find('div', class_='css-1b6t4dn').text  # Example class, inspect for correct class\n",
    "            price = product.find('div', class_='css-o5uqvq').text  # Example class, inspect for correct class\n",
    "            rating = product.find('span', class_='css-t70v7i').text if product.find('span', class_='css-t70v7i') else 'No Rating'  # Example class\n",
    "            \n",
    "            # Add product details to the list\n",
    "            product_list.append({\n",
    "                'Product Name': name,\n",
    "                'Price': price,\n",
    "                'Rating': rating\n",
    "            })\n",
    "        except AttributeError:\n",
    "            # Skip product if there is an issue with extraction\n",
    "            continue\n",
    "    \n",
    "    # Create a DataFrame from the list of products\n",
    "    df = pd.DataFrame(product_list)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Fifth cell: Function to scrape pages dynamically using Selenium (if needed)\n",
    "def scrape_tokopedia_with_selenium(query):\n",
    "    url = construct_search_url(query)\n",
    "    \n",
    "    # Initialize Selenium WebDriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the page to load fully (you might need to adjust this)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    \n",
    "    # Same parsing process as with requests, but now using the rendered content from Selenium\n",
    "    products = soup.find_all('div', class_='css-1sn1xa2')  # Example class\n",
    "    \n",
    "    product_list = []\n",
    "    \n",
    "    for product in products:\n",
    "        try:\n",
    "            name = product.find('div', class_='css-1b6t4dn').text\n",
    "            price = product.find('div', class_='css-o5uqvq').text\n",
    "            rating = product.find('span', class_='css-t70v7i').text if product.find('span', class_='css-t70v7i') else 'No Rating'\n",
    "            \n",
    "            product_list.append({\n",
    "                'Product Name': name,\n",
    "                'Price': price,\n",
    "                'Rating': rating\n",
    "            })\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    \n",
    "    df = pd.DataFrame(product_list)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Sixth cell: Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = \"laptop\"  # Example search query, can be changed\n",
    "    df = scrape_tokopedia(search_query)\n",
    "    \n",
    "    if df is not None:\n",
    "        print(\"Scraped Products:\")\n",
    "        display(df)  # Display DataFrame in Jupyter Notebook\n",
    "        df.to_csv('tokopedia_products.csv', index=False)  # Export to CSV\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
